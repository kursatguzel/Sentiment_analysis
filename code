# -*- coding: utf-8 -*-
"""dogaldilislemefinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y11a5nqCyrTjdYTrmVZfaskBA9BMf70-

# Bağlantı
"""

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive
!ls

"""# Kütüphaneler"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
import string #noktalama işaretleri
import re
import nltk #etkisiz kelimeler
import numpy
import csv
import pandas as pd
import os, glob
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from bs4 import BeautifulSoup
import requests

"""# Veri ayrıştırma/ Parse

Bot engeline karşı
"""

headersparam = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:78.0) Gecko/20100101 Firefox/78.0"}

"""Siteden format değiştirme ve çekme"""

# sadece **** tüm yorumlar sayfası için çalışır!!!
r = requests.get("website_url", headers=headersparam)
yorum =r.content
soup = BeautifulSoup(yorum,"html.parser")

"""İlk pars işlemleri"""

# Farklı siteler için "div" ve "attrs" kısımları o sayfaya göre özelleştirilmeli
a= soup.find_all("div",attrs={"class":"pr-rnr-com"})

"""Ayrıştırılmış verileri sıra ile diziye kayıt etme"""

Liste=[]
for i in a:
    b = i.find_all("div",attrs={"class":"rnr-com-w"})
    for j in b:
        yorum1=j.find("div",attrs={"class":"rnr-com-tx"}).text.strip()
        #begeni = i(" ")
        #tarih=i.find("span",attrs={"class":"rnr-com-usr"}).span.next_sibling

        #satisdurumu=i.find("span",attrs={"class":"rnr-com-usr"}).next_sibling.text

        #alici=i.find("span",attrs={"class":"rnr-com-usr"}).span.previous_sibling

        #satici=i.find("span",attrs={"class":"seller-name-info"}).text

        #begenisayisi=i.find("div",attrs={"class":"rnr-com-like"}).span.next_sibling.text

        #Liste.append([yorum1,tarih,satisdurumu,alici,satici,begenisayisi])
        Liste.append([yorum1])

"""Diziyi dataframe etme"""

df = pd.DataFrame(Liste)
#df.columns=["Yorumlar","Tarih","Durum","Müşteri","Satıcı","Beğeni"]
df.columns=["Yorumlar"]

"""Doğruluk testi"""

df

"""Excel formatında kayıt etme"""

df.to_excel('/content/drive/hamveri.xlsx')

"""# Veri Seti Yükleme"""

# csv den veri yükleme

from openpyxl import Workbook,load_workbook 
yorumlar = pd.read_excel('/content/drive/verislenmis.xlsx')
yorumlartest = pd.read_excel('/content/drive/hamveri.xlsx')

#ilk beş girdi görmek için
yorumlar.head() 
#yorumlartest.head()

"""# Veri Seti Önişleme"""

nltk.download('stopwords') # önemliiiii
noktalama = string.punctuation
etkisiz = stopwords.words('turkish')
print(noktalama)
print(etkisiz)

"""# Eğitim verileri"""

#yorumdizim=[]
#likedizim=[]
for d in yorumlar['Yorumlar'].head():
    print(d+ '\n------------------------------')
    # Etkisiz kelimeleri atmak
    temp= ' '
    for word in d.split():
        if word not in etkisiz and not word.isnumeric():
            temp += word + ' '
      #yorumdizim.append(temp)
      #yorumcsv = pd.DataFrame(yorumdizim)
      #data1 = yorumcsv.to_excel
    print(temp+ '\n********************')

"""Noktalama işaretleri silme"""

for d in yorumlar['Yorumlar'].head():
    print(d+ '\n------------------------------')
    # Noktalama işaretleri atma
    temp= ' '
    for word in d:
        if word not in noktalama:
            temp += word
            #yorumdizim.append(temp)
    print(temp+ '\n********************')
    d= temp

"""# Test verileri"""

for d in yorumlartest['Yorumlar'].head():

    print(d+ '\n------------------------------')
    # Etkisiz kelimeleri atmak
    temp= ' '
    for word in d.split():
        if word not in etkisiz and not word.isnumeric():
            temp += word + ' '
      #yorumdizim.append(temp)
      #yorumcsv = pd.DataFrame(yorumdizim)
      #data1 = yorumcsv.to_excel
        
    print(temp+ '\n********************')

for d in yorumlartest['Yorumlar'].head():
    print(d+ '\n------------------------------')
    # Noktalama işaretleri atma
    temp= ''
    for word in d:
        if word not in noktalama:
            temp += word
            #yorumdizim.append(temp)
    print(temp+ '\n********************')
    d= temp

"""# Verileri kayıt etme"""

#df = pd.DataFrame(yorumdizim)
#df.columns=["Yorumlar","Tarih"]
#print(df)

yorumlar.to_csv('/content/drive/cleaned.csv', index = False)
yorumlartest.to_csv('/content/drive/cleanedtest.csv', index = False)

yorumlar = pd.read_csv('/content/drive/cleaned.csv')
yorumlartest = pd.read_csv('/content/drive/cleanedtest.csv')

print(yorumlar.head())

"""# Veri Setini Bölme"""

# Arındırılmış veriyi train ve test kümelerine ayırıyoruz
# Eğitim ve test olarak ayıracağız. train=eğitim, test=test

X_train, X_test, y_train, y_test = train_test_split(yorumlar['Yorumlar'].values.astype('U'),yorumlar['begeni'].values.astype('U'), test_size=0.1, random_state=42)
# XX_train işimize yaramıyor.
XX_train, XX_test = train_test_split(yorumlartest['Yorumlar'].values.astype('U'), test_size=0.9, random_state=42)
print(X_train.shape)
print(XX_test.shape)

"""# Sayma Vektörü Oluşturma"""

# Train kümesindeki cümlelerin sayma vektöelerini çıkarıyoruz
count_vect = CountVectorizer(max_features = 20000) # Burası önemli ram için
X_train_counts = count_vect.fit_transform(X_train)
print(X_train_counts.shape)

"""Tf*Idf vektörü oluşturma
sadece o yoruma özgü(temsil edecek kelimler olmalı)
"""

# Train kümesindeki cümlelerin TF*IDF vektörlerini sayma vekttörlerinden oluşturuyoruz
# Tf= terim sıklığı, IDF= bir kelimenin dökümanda kaç kere geçtiği
# TF*IDF bütün olarak; bir kelimenin bir döküman içinde ki önemini gösterir
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
print(X_train_tfidf.shape)

"""# Naive Bayes Model Eğitimi"""

# Çok modlu Naive Bayes Sınıflandırıcısı eğitiyoruz
clf = MultinomialNB().fit(X_train_tfidf, y_train)
XX_test_counts = count_vect.transform(XX_test)
XX_test_tfidf = tfidf_transformer.transform(XX_test_counts)

"""# Model Performansı Ölçme"""

# Sınıflandırıcı ile test seti üzerindeki tahminleme yapıyoruz
y_pred = clf.predict(XX_test_tfidf)
for review, sentiment in zip(XX_test[:], y_pred[:]):
  print('%r => %s' % (review, sentiment))

"""# Grafik"""

# Performans sonuçları
# Veri seti bölme aşamasında kümelemeler eşit olmadığı için bu kısım hata verebilir!!
from sklearn.metrics import accuracy_score

print(accuracy_score(XX_test, y_pred))

